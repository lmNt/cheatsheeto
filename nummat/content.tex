\begin{center}
     \Large{\textbf{OptCtrl Cheat Sh33t}} \\
\end{center}

\section{Basics}
%\subsection{Differentiation}
%\vspace{0.1cm}
%\begin{tabular}{@{}ll@{}}
%verb!Gradient!    & $(\nabla f)(\mbb{x}) = \begin{bmatrix}\frac{\partial}{\partial x_1} f(\mbb{x}) \\ \vdots \\ %\frac{\partial}{\partial x_n} f(\mbb{x})\end{bmatrix}$ \\ \\
%\verb!Hessian!  & $(\nabla^2 f)(\mbb{x}) = \begin{bmatrix}\frac{\partial^2}{\partial x_1^2} f(\mbb{x}) & \cdots & %\frac{\partial^2}{\partial x_1\partial x_n} f(\mbb{x}) \\ \vdots & & \vdots \\ \frac{\partial^2}{\partial x_n\partia%l x_1} f(\mbb{x}) & \cdots &  \frac{\partial^2}{\partial x_n^2} f(\mbb{x})\end{bmatrix}$ \\ \\
%\verb!Jacobian! & $\frac{\partial}{\partial \mbb{x}} \mbb{f}(\mbb{x}) = \begin{bmatrix}\frac{\partial}{\partial x_1} %f_1(\mbb{x}) & \cdots & \frac{\partial}{\partial x_n} f_1(\mbb{x}) \\ \vdots & & \vdots \\ \frac{\partial}{\partial %x_1} f_n(\mbb{x}) & \cdots &  \frac{\partial}{\partial x_n} f_n(\mbb{x})\end{bmatrix}$ 
%\end{tabular}
%\vspace{0.2cm}

\textsc{Table of gradients and derivatives}\\
\vspace{0.1cm}
\begin{align*}
\nabla_x \,\mbb{Ax} &= \mbb{A}^T \\
\nabla_x \,\mbb{x}^T\mbb{A} &= \mbb{A}\\
\nabla_x \,\mbb{x}^T\mbb{x} &= 2\mbb{x}\\
\nabla_x \,\mbb{x}^T\mbb{Ax} &= \mbb{Ax} + \mbb{A}^T\mbb{x}\\
\nabla_x \,\mbb{b}^T\mbb{x} &= \mbb{b}\\
\nabla_x \,(\mbb{Ax}-\mbb{b}) &= \mbb{A}^T\\
\nabla_x \,(\mbb{x}+\mbb{y})^T \mbb{A}(\mbb{x}+\mbb{y}) &= (\mbb{A}+\mbb{A}^T)(\mbb{x}+\mbb{y})
\end{align*}
\vspace{0.1cm}

\textsc{Formulations}\\
\vspace{0.1cm}
\verb!Linear program!\\
$\min \mbb{c}^T\mbb{x}$, s.t. $\mbb{Ax}=\mbb{b}$, $\mbb{x}\geq \mbb{0}$ \vspace{0.1cm}

\verb!Quadratic program!\\
$\min \frac{1}{2}\mbb{x}^T\mbb{Q}\mbb{x} + \mbb{c}^T\mbb{x}$, s.t. $\mbb{Ax}\leq\mbb{b}$, $\mbb{Ex} = \mbb{d}$ \vspace{0.1cm}

\section{Static unconstrained optimization}
\newlength{\MyLen}

\verb!First order necessary optimality condition! \\ 
$(\nabla f)(\mbb{x}^*)=0$ \vspace{0.1cm}

\verb!Second order necessary optimality condition! \\ 
$(\nabla^2 f)(\mbb{x}^*)\geq 0$ \vspace{0.1cm}

\verb!Second order sufficient optimality condition! \\ 
If $(\nabla f)(\mbb{x}^*)=0$ and $(\nabla^2 f)(\mbb{x}^*) > 0$, then $\mbb{x}^*$ is a strict local minimizer. \vspace{0.1cm}

\verb!Theorem 2.4! \\
If $f(\mbb{x})$ is convex, any local minimizer $\mbb{x}^*$ is a global minimizer. If $f(\mbb{x})\in C^1(X_{ad})$, then any stationary point is a global minimizer.  \vspace{0.1cm}

\verb!Convexity!\\
If $f(\cdot)$ is convex any local miminizer is a global minimizer.\\
If $f(\cdot)$ is convex any strict local minim. is a global unique. \\
If $f(\cdot)$ is strictly convex any local minim. is a global unique. \\

\subsection{Numerical minimization algorithms}
%\settowidth{\MyLen}{\texttt{multicol} }

\subsubsection{Schematic line search method}
\begin{enumerate}
\item Input start value $\mbb{x}_0$ and stopping criteria $\epsilon$.
\item Initialize $k=0$.
\item repeat
\begin{enumerate}
\item Search direction $\mbb{s}_k$
\item Step length $\alpha_k$
\item $\mbb{x}_{k+1} = \mbb{x}_k + \alpha_k\mbb{s}_k$
\item $k=k+1$
\end{enumerate}
\item until $||f(\mbb{x_{k+1})}||\leq \epsilon$
\end{enumerate}

\subsection{Determination of step length}
\subsubsection{Armijo conditions}
Usually $\epsilon_0 \leq 0.01$, $\epsilon_1 > 1$:
\begin{align*}
\text{upper bound: }& f(\mbb{x}_k + \alpha_k \mbb{s}_k) \leq f(\mbb{x}_k) + \epsilon_0 \alpha_k \mbb{s}_k^T (\nabla f)(\mbb{x}_k)\\
\text{lower bound: }& f(\mbb{x}_k + \alpha_k \mbb{s}_k) \geq f(\mbb{x}_k) + \epsilon_0 \epsilon_1 \alpha_k \mbb{s}_k^T (\nabla f)(\mbb{x}_k)
\end{align*}
\begin{enumerate}
\item If upper bound satisfied, increase $\alpha_k$ by $\epsilon_1$ until upper bound violated.
\item If lower bound violated, decrease $\alpha_k$ by $\epsilon_1$ until lower bound satisfied.
\item Assign $\alpha_k$ as step length
\end{enumerate}

\subsubsection{Wolfe conditions}
$\epsilon_1 \in (0,1)$, $\epsilon_2 \in (e_1,1)$, typically $\epsilon_2=0.9$ (Newton) or $\epsilon_2=0.1$ (Conj. grad.). (\textcolor{red}{Strong Wolfe}):
\begin{align*}
\text{upper bound: }& f(\mbb{x}_k + \alpha_k \mbb{s}_k) \leq f(\mbb{x}_k) + \epsilon_1 \alpha_k \mbb{s}_k^T (\nabla f)(\mbb{x}_k)\\
\text{lower bound: }& \textcolor{red}{|}\mbb{s}_k^T(\nabla f)(\mbb{x}_k + \alpha_k \mbb{s}_k)\textcolor{red}{|} \geq \textcolor{red}{\leq} \epsilon_2 \textcolor{red}{|}\mbb{s}_k^T (\nabla f)(\mbb{x}_k)\textcolor{red}{|}
\end{align*}

\subsubsection{Backtracking}
+ Newton \\
- quasi-Newton, conj. Grad.\vspace{0.1cm}

$\alpha_k^0 > 0$ ($\alpha_k^0=1$ Newton), $\rho \in (0,1)$, $\epsilon_0 \in (0,1)$\\
Repeat:\\
\hspace{0.1cm} $\alpha_k \leftarrow \rho \alpha_k$\\
until $f(\mbb{x_k} + \alpha_k s_k) \leq f(\mbb{x_k}) + \epsilon_0 \alpha_0 s_k^T (\nabla f)(\mbb{x_k})$

\subsubsection{Others}
\textsc{Nested intervals}\\
+ easily implementable, numerically robust \\ 
- larger number of iteration steps \vspace{0.1cm}

\textsc{Quadratic interpolation}\\
+ very efficient \\


\subsection{Numerical minimization methods}
\subsubsection{Line search methods}
\verb!Steepest descent! \\
$\mbb{s}_k = -(\nabla f)(\mbb{x}_k)$ \vspace{0.1cm}

+ Simple / low computation (no explicit evaluation of Hessian needed) \\
+ Good convergence for starting values not close to local minimizer \\
- Slow convergence depending on conditioning (linear convergence) \vspace{0.1cm}

\verb!Conjugated gradient! \\
+ well understood convergence for linear and quadratic probs. \\
- not for nonlinear \vspace{0.1cm}

$\mbb{s}_k = -(\nabla f)(\mbb{x}_k) + \beta_k \mbb{s}_{k-1}, k\geq 1$\\
$\mbb{s}_0 = -(\nabla f)(\mbb{x}_0)$ \\
Fletcher-Reeves: \\
$\beta_k^{FR} = \frac{(\nabla f)^T(\mbb{x}_k)(\nabla f)(\mbb{x}_k)}{(\nabla f)^T(\mbb{x}_{k-1})(\nabla f)(\mbb{x}_{k-1})}$ \vspace{0.4cm}

\verb!Newton method! \\
$\mbb{s}_k = -(\nabla^2 f)^{-1}(\mbb{x}_k)(\nabla f)(\mbb{x}_k)$\vspace{0.1cm}

+ quadratic convergence if Hessian is positive definite\\
- loss of pos. def. of the Hessian if $\mbb{x}$ not sufficiently close to $\mbb{x}^*$ \\
- requires evaluation and inversion of Hessian \vspace{0.1cm}

%$\mbb{x}_{k+1} = \mbb{x}_k - (\nabla^2 f)^{-1}(\mbb{x}_k)(\nabla f)(\mbb{x_k})$\vspace{0.1cm}

\verb!Quasi Newton method! \\
$\mbb{s}_k = -B_k^{-1}(\nabla f)(\mbb{x}_k)$ \\
$\mbb{p}_k = \mbb{x}_{k+1} - \mbb{x}_k, \qquad \mbb{q}_k = (\nabla f)(\mbb{x}_{k+1}) - (\nabla f)(\mbb{x}_k)$ \\
$H_{k+1}=H_k + \frac{(p_k-H_k q_k)(p_k - H_k q_k)^T}{q_k^T(p_k - H_k q_k)}$\vspace{0.1cm}

$H_{k+1}$ is inverting $B_{k+1}$.\\

BFGS with: \\
$H_{k+1}=\left(E-\frac{p_kq_k^T}{q_k^T p_k}\right)H\left(E-\frac{q_k p_k^T}{q_k^T p_k}\right)+\frac{p_kp_k^T}{q_k^T p_k}$\vspace{0.1cm}

+ less computationally expensive than Newton \\
- more iterations \vspace{0.1cm}

\subsubsection{Trust region methods}
Define region around current search iterate to trust the quadratic model being an adequate approximation of the objective function.\vspace{0.1cm}

$m(\mbb{s}_k) = f(x_k)+s_k^T(\nabla f)(x_k)+\frac{1}{2}s_k^TB_ks_k \approx f(\mbb{x}_k + \mbb{s}_k)$\vspace{0.1cm}

Solution $s_k^*$ is minimizer of $m(s_k)$ in radius $\Delta_k$. Step length and search direction are calculated simultaneously. Depending on the ratio of actual reduction and predicted reduction the region is shrunk if ratio is $<0 \ll 1$ . For ratios $\approx 1$ region may be expanded.

\subsubsection{Direct search methods}
Derivative free methods, so no explicit knowledge of gradient or Hessian of $f(\cdot)$ is needed to find minimum. \vspace{0.1cm}

\verb!Simplex algorithm of Nelder and Mead! \\
Build a simplex, which is a convex hull, spanned by $n+1$ points. Take worst vertex (highest value) and replace with new point which lies on the line spanned by the highest value through the centroid $\bar{x}$ of the simplex.\\
$x_k^{ref} = \bar{x}_k + \alpha(\bar{x}_k-x_{k,max})$


\section{Static constrained optimization}
\subsection{Equality constrained}
\verb!Regular point! \\  $\mbb{x}^*$ satisfying $\mbb{g}(\mbb{x}^*)=0$ is a regular point, if $\text{rank}(\nabla \mbb{g})(\mbb{x}^*)=\text{rank}[(\nabla g_1),\cdots,(\nabla g_p)](\mbb{x}^*) = p$\vspace{0.1cm}

\verb!Tangent space! \\ $\mathcal{T}_{x^*}\mathcal{M}=\{\mbb{d}\in R^n : (\nabla g_j)^T(\mbb{x}^*)\mbb{d} = 0, j=1,...,p\}$ \vspace{0.1cm}

\verb!Lemma 3.1! \\ All $\mbb{d}$ fulfilling $(\nabla\mbb{g})^T(\mbb{x}^*)\mbb{d} = 0$ must also satisfy $(\nabla\mbb{f})^T(\mbb{x}^*)\mbb{d} = 0$ \vspace{0.1cm}

\verb!First order necessary optimality condition! \\
$(\nabla f)(\mbb{x}^*) + (\nabla g)(\mbb{x}^*)\mbb{\lambda}^*=\mbb{0}$ or \\ $\nabla_{\mbb{x}} l(\mbb{x}^*,\mbb{\lambda}^*) = \mbb{0}$ and $\nabla_{\mbb{\lambda}} l(\mbb{x}^*,\mbb{\lambda}^*) = \mbb{0}$ \vspace{0.1cm}

\verb!Langrangian!\\
$l(\mbb{x}^*,\mbb{\lambda}^*) = f(\mbb{x}^*) + (\mbb{\lambda}^*)^T \mbb{g}(\mbb{x})$\vspace{0.1cm}

\verb!Second order necessary optimality condition! \\
$\mbb{d}^T(\nabla^2 l)(\mbb{x}^*, \mbb{\lambda}^*)\mbb{d} \geq 0$ \vspace{0.2cm}

\verb!Second order sufficient optimality condition! \\
$\nabla_{\mbb{x}} l(\mbb{x}^*,\mbb{\lambda}^*) = \mbb{0}$,  $\nabla_{\mbb{\lambda}} l(\mbb{x}^*,\mbb{\lambda}^*)=0$ and $\mbb{d}^T(\nabla^2 l)(\mbb{x}^*, \mbb{\lambda}^*)\mbb{d} > 0$ \vspace{0.2cm}

\verb!Sensitivity (eq.constr.) - UNFINISHED(??)!\\
$\frac{\partial}{\partial\mbb{c}}f(\mbb{x}(\mbb{c}))|_{\mbb{c}=0} = -(\mbb{\lambda}^*)^T$ \vspace{0.3cm}

\subsection{Inequality constrainted}
\verb!Regular point (LICQ)! \\  $\mbb{x}^*$ satisfying $\mbb{g}(\mbb{x}^*)=0$ and $\mbb{h}(\mbb{x}^*)\leq 0$ is a regular point, if the gradient vectors  $(\nabla \mbb{g})$ and $(\nabla \mbb{h})$ (only active ieq.) are linearly independent. \vspace{0.2cm}

\verb!KKT first order necessary optimality condition! \\
$(\nabla f)(\mbb{x}^*) + (\nabla g)(\mbb{x}^*)\mbb{\lambda}^*+(\nabla h)(\mbb{x}^*)\mbb{\mu}^*=\mbb{0}$ \\ 
\hspace{3.78cm}$\mbb{h}^T(\mbb{x}^*)\mbb{\mu}^* = 0$ \\
\hspace{4.78cm}$\mbb{\mu}^* \geq \mbb{0}$ \\
\vspace{0.2cm}

\verb!Complementary slackness condition! \\
$h_l(\mbb{x}^*) < 0$ $\rightarrow$ \,$\mu_l^*=0$ (inactive)\\
$h_l(\mbb{x}^*) = 0$ $\rightarrow$ \,$\mu_l^*>0$ (active)\vspace{0.2cm}

\verb!KKT second order necessary optimality condition! \\
$\displaystyle \mbb{d}^T(\nabla^2 l)(\mbb{x}^*, \mbb{\lambda}^*, \mbb{\mu}^*)\mbb{d} (\textcolor{red}{>})\geq 0$ (\textcolor{red}{sufficient}) with \vspace{0.1cm}

$\mbb{d} \in \mathcal{T}_{x^*}\mathcal{M} = \{\mbb{d}\in R^n : (\nabla g_j)^T(\mbb{x}^*)\mbb{d}=0,\, j=1,\cdots,p, \,(\nabla h_l)^T(\mbb{x}^*)\mbb{d}=0, \forall l \in \mathcal{A}_{ieq}(\mbb{x})\}$ \vspace{0.1cm}

and the Lagrangian \\
$l(\mbb{x}^*,\mbb{\lambda}^*,\mbb{\mu}^*) = f(\mbb{x}^*) + (\mbb{\lambda}^*)^T \mbb{g}(\mbb{x}) + (\mbb{\mu}^*)^T\mbb{h}(\mbb{x}^*)$\vspace{0.2cm}

\verb!Theorem 3.9! \\
The static constrained opt. problem is convex if $f(\mbb{x})$ is convex on the admissible set and $\mbb{g}(\mbb{x})$ is linear and $\mbb{h}(\mbb{x})$ is convex. \vspace{0.2cm}

\verb!Sensitivity (ieq.constr.) - UNFINISHED(??)!\\
$\frac{\partial}{\partial\mbb{c}}f(\mbb{x}(\mbb{c},\mbb{d}))|_{\mbb{c}=0,\mbb{d}=0} = -(\mbb{\lambda}^*)^T$ and \\ $\frac{\partial}{\partial\mbb{d}}f(\mbb{x}(\mbb{c},\mbb{d}))|_{\mbb{c}=0,\mbb{d}=0} = -(\mbb{\mu}^*)^T$\vspace{0.3cm}


\section{Numerical optimization algorithms}
\subsection{Primal methods}
Primal methods work on the orignal problem by searching a solution in the feasible space, i.e. active constraints.
+ Each point is feasible - no problem with premature termination \\
+ Convergence to at least a local minimum can be guaranteed.\\
+ Do not rely on e.g. convexity\\
- Require initilization phase to determine feasible starting point \\
- Computational issues may arise due to restriction on feasible space \\
- Problems with inequality constraints may let the algorithm fail \\

\subsubsection{Active set methods}
\begin{enumerate}
\item Determine working set from feasible starting point, solve opt. problem over the working surface
\item Add newly encountered ieq.constr. to the working set, but do not drop.
\item Determine Lagrange multipliers if minimum in $f(\mbb{x})$ is found. If they are nonnegative, the solution is (locally) optimal.
\item Drop inequality constraints with negative Lagrange multipliers and restart with the new working set.
\end{enumerate}
\vspace{0.1cm}
+ Often very effective for static constrained optimization (rare zigzagging) \\
- Each iterate has to be exact to the intermediate optimization problem to ensure the signs of Lagrange multipliers are correct \\


\subsubsection{Gradient projection methods}
Move along the projected $-(\nabla f)(\mbb{x}_k)$ to a certain point on the tangent plane and then perpendicular to it, to reach a feasible point on the manifold $\mathcal{M}$.\vspace{0.1cm}

- Complex implementation
- Recomputation of projection matrix in each step \\
- Previously inactive inequality constraints may be violated when moving in direction of projected gradient (use interpolation - trial and error or relax constraints) \\

\subsection{Penalty and barrier methods}
Directly work in $n$-dimensional space of decision variables.
+ easy to implement but still ensure (slow) convergence.\\
+ Lagrange multipliers can be recovered

\subsubsection{Penalty methods}
Approximating constrained optimization problem by unconstrained optimization problem:\vspace{0.1cm}

$\min f(\mbb{x})+cP(\mbb{x})$, $c=const.>0$, $P(\mbb{x})\geq 0$ for all $\mbb{x}$ and $P(\mbb{x}) = 0$ only if $\mbb{x}\in X_{ad}$. \vspace{0.1cm}

+ Superlinear convergence to feasible point...\\
- ... which may be far from optimal \\
- Ill-cond. or round-off errors may yield slow convergence or premature termination
\vspace{0.1cm}

\subsubsection{Barrier methods (interior point)}
Constrained opt. problem is transformed into (sequence of) unconstrained problem to create barriers which prevent the iterates to leave the $X_{ad}$.\vspace{0.1cm}

+ Competetive algorithm \\
- Finding feasible starting point satisfying ieq. constraints may be difficult (nonlinear problems) \\
- Ill-conditioning or round-off errors (near $X_{ad}$ boundary) of $c_k$ may arise \vspace{0.1cm}

\subsection{Sequential quadratic programming}
Solving a sequence of quadratic programs in Newton manner. From Newton method it is known that: \\
$(\nabla F)(x_k)r_k = -F(x_k)$, where $x_{k+1} = x_k +r_k$ \vspace{0.1cm}

This can be viewed as:
$\min_{p_k} f(x_k) + (\nabla f)^T(x_k)p_k + \frac{1}{2}p_k^T L(x_k, \lambda_k)p_k$ \\ s.t $(\nabla g)^T(x_k)p_k + g(x_k) = 0$ \\
So $p_k=0$ yields $x^*, \lambda^*$ that satisfies KKT conditions of the minmization problem. \\
Active set methods can be used to solve SQP. Convergence can be ensured if start is close to the local minimizer or else the Hessian of the Langrangian can become indefinite, but however this can be overcome by using a (modified) BFGS method or global SQP by introducing step length $alpha_k$ to ensure convergence from remote points.

\section{Dynamic optimization}
\subsection{Unconstrained}
\verb!Cost functional in Bolzano form! \\
$J(\mbb{u}) = \underbrace{\varphi(t_e, \mbb{x}(t_e))}_\text{terminal constraint} + \int_{t_0}^{t_e} l(t,\mbb{x}(t),\mbb{u}(t))dt$ \vspace{0.1cm}

\verb!Dynamic system! \\
$\dot{\mbb{x}} = \mbb{f}(t,\mbb{x},\mbb{u}), \,\, t\geq t_0, \,\, \mbb{x}(t_0) = \mbb{x}_0 \in R^n$ \vspace{0.1cm}

%\verb!Feasible control and feasible pair! \\
%$\mbb{u}(t)$ is feasible if \\ 
%(i) the solution $\mbb{x}(t) = \mbb{x}(t;\mbb{x}_0,\mbb{u}(t))$ is defined on $t \in %[t_0,t_e]$ \\
%(ii) $\mbb{u}(t)$ and $\mbb{x}(t;\mbb{x}_0,\mbb{u}(t))$ satisfy all constraints for $t %\in [t_0,t_e]$. \vspace{0.1cm}

\verb!Gateaux derivative (holds for any norm on X)! \\
$\delta J(\mbb{x},\mbb{\xi}) = \lim_{\eta\rightarrow 0} \frac{J(\mbb{x} + \eta\mbb{\xi})-J(\mbb{x})}{\eta}=\frac{\partial}{\partial \eta} J(\mbb{x} + \eta\mbb{\xi})|_{\eta=0}$ \vspace{0.1cm}

\verb!First order necessary optimality condition! \\
$\delta J(\mbb{x}^*,\mbb{\xi}) = 0$ \vspace{0.1cm}

%\verb!Fundamental lemma of variational calculus! \\
%blaaaaaaa \vspace{0.1cm}

\verb!Euler-Lagrange equations! \\
Given a functional in Lagrange form and $\mbb{x}^*(t)$ is a local minimizer, then $\mbb{x}^*(t)$ fulfills for all $t\in [t_0,t_e]$:\\
$\frac{\partial}{\partial t}(\nabla_{\dot{\mbb{x}}} l)(t,\mbb{x}^*(t), \dot{\mbb{x}}^*(t))-(\nabla_{\mbb{x}} l)(t,\mbb{x}^*(t), \dot{\mbb{x}}^*(t)) = \mbb{0}$ \vspace{0.1cm}

(i) If $l=l(\mbb{x}(t),\dot{\mbb{x}}(t))$ (independent of $t$) then \\
$H(\mbb{x},\dot{\mbb{x}})=(\nabla_{\dot{x}}l)(\mbb{x},\dot{\mbb{x}})\dot{\mbb{x}} - l(\mbb{x},\dot{\mbb{x}}) = c$ \\
is an invariant of the E-L eq. and remains const. along $\mbb{x}^*(t)$. \vspace{0.1cm}

(ii) If $l=l(t,\dot{\mbb{x}}(t))$ (independent of $\mbb{x}(t)$) then \\
$\frac{\partial}{\partial t}(\nabla_{\dot{\mbb{x}}} l)^T(t,\dot{\mbb{x}}) = \mbb{0}$ \\
is an invariant of the E-L eq. \vspace{0.1cm}

\verb!Legendre condition! \\
$\mbb{x}^*(t)$ is a local minimizer if for all $t\in [t_0,t_e]$: \\
$(\nabla_{\dot{\mbb{x}}}^2 l)(t,\mbb{x}^*(t), \dot{\mbb{x}}^*(t)) \geq 0$ \vspace{0.1cm}

\verb!Euler-Lagrange eq. (free end-point) - Transvers. cond.! \\
$[(\nabla_{\dot{\mbb{x}}}l)(t,\mbb{x}(t),\dot{\mbb{x}}(t)) + (\nabla_{\mbb{x}} \varphi)(t,\mbb{x}(t))]_{t=t_e^*, \mbb{x}=\mbb{x}^*} = \mbb{0}$ \vspace{0.1cm}

$[ l(t,\mbb{x}(t),\dot{\mbb{x}}(t)) - (\dot{\mbb{x}})^T(\nabla_{\dot{\mbb{x}}}l)(t,\mbb{x}(t),\dot{\mbb{x}}(t))+ \frac{d}{dt}\varphi(t,\mbb{x}(t))]_{t=t_e^*, \mbb{x}=\mbb{x}^*} = 0$ \vspace{0.1cm}

\subsection{Constrained}
\subsubsection{Equality constrained}
\verb!Existence of Lagrange multipliers (single eq. constr.)! \\
$\delta J(\mbb{x}^*, \mbb{\xi}) + \lambda\delta G(\mbb{x}^*, \mbb{\xi}) = 0, \,\,\,\, \forall \mbb{\xi} \in X$\vspace{0.2cm}

\verb!Existence of Lagrange multipliers (multiple eq. constr.)! \\
$\delta J(\mbb{x}^*, \mbb{\xi}) + [\delta G_1(\mbb{x}^*, \mbb{\xi}) \cdots \delta G_p(\mbb{x}^*, \mbb{\xi})] \mbb{\lambda} = 0, \,\,\,\, \forall \mbb{\xi} \in X$\vspace{0.2cm}

\subsubsection{Inequality constrained}
\verb!Existence of Lagrange multipliers (multiple ieq. constr.)! \\
$\delta J(\mbb{x}^*, \mbb{\xi}) + [\delta G_1(\mbb{x}^*, \mbb{\xi}) \cdots \delta G_p(\mbb{x}^*, \mbb{\xi})] \mbb{\mu} = 0, \,\,\,\, \forall \mbb{\xi} \in X$

\hspace{3.15cm}$(G_j(\mbb{x}^*) - s_j)\mu_j = 0$ 

\hspace{4.99cm}$\mu_j \geq 0$

\verb!Complementary slackness condition! \\
For constraints $G_j(\mbb{x}^*)=s_j$, \\
(i)  \,$(G_j(\mbb{x}^*) - s_j) < 0$ implies $\mu_j = 0$ \\
(ii) $(G_j(\mbb{x}^*) - s_j) = 0$ implies $\mu_j > 0$ \vspace{0.2cm}

\subsubsection{Isoperimetrically constrained}

\section{Optimal control}
\verb!Hamiltonian function! \\
$H(t,\mbb{x},\mbb{u},\mbb{\lambda}) = l(t,\mbb{x},\mbb{u}) + \mbb{\lambda}^T \mbb{f}(t,\mbb{x},\mbb{u})$ \vspace{0.2cm}

\subsection{Unconstrained}
\subsubsection{Fixed-time, free-endpoint}
Two-point-BVP since initial state in $\mbb{x}^*(t)$ and final state in the adjoint state $\mbb{\lambda}^*(t)$ are given. \vspace{0.1cm}

$\dot{\mbb{x}}^* = (\nabla_{\lambda} H)(t,\mbb{x}^*,\mbb{u}^*,\mbb{\lambda}^*), \,\,\, \mbb{x}^*(t_0) = \mbb{x}_0$ \\
$\dot{\mbb{\lambda}}^* = -(\nabla_{x} H)(t,\mbb{x}^*,\mbb{u}^*,\mbb{\lambda}^*), \,\,\, \mbb{\lambda}^*(t_e) = \mbb{0}$ \\
$\,\,\,\,\mbb{0} = (\nabla_{u} H)(t,\mbb{x}^*,\mbb{u}^*,\mbb{\lambda}^*)$ \vspace{0.1cm}

$\mbb{u}^*(t)$ must be a stationary point of the Hamiltonian, e.g. $H(t,\mbb{x}^*,\mbb{u}^*,\mbb{\lambda}^*)=const.$ for the triple to be a local minimizer of $J$. \vspace{0.1cm}

Legendre condition:\\
$(\nabla_{u}^2 H)(t,\mbb{x}^*,\mbb{u}^*,\mbb{\lambda}^*) \geq 0$ (positive semi-definite) \vspace{0.1cm}

With terminal cost:\\
$\mbb{\lambda}^*(t_e) = (\nabla_x \varphi)(t_e, \mbb{x}^*(t_e))$\vspace{0.3cm}

\subsubsection{Free-time, fixed-endpoint}
Minimization problem in Bolzano form, with $p$ equality constraints $G_k(t_e,\mbb{u})=\psi_k(t_e,\mbb{x}(t_e)) = 0$: \vspace{0.1cm}

$\dot{\mbb{x}}^* = (\nabla_{\lambda} H)(t,\mbb{x}^*,\mbb{u}^*,\mbb{\lambda}^*), \,\,\, \mbb{x}^*(t_0) = \mbb{x}_0$ \\
$\dot{\mbb{\lambda}}^* = -(\nabla_{x} H)(t,\mbb{x}^*,\mbb{u}^*,\mbb{\lambda}^*), \,\,\, \mbb{\lambda}^*(t_e^*) = (\nabla_{x_e} \phi)(t_e^*, \mbb{x}^*(t_e^*),\mbb{\mu}^*)$ \\
$\,\,\,\,\mbb{0} = (\nabla_{u} H)(t,\mbb{x}^*,\mbb{u}^*,\mbb{\lambda}^*)$ \vspace{0.1cm}

with

$\phi(t_e,\mbb{x}(t_e),\mbb{\mu}) = \varphi(t_e,\mbb{x}(t_e)) + \mbb{\mu}^T \psi (t_e,\mbb{x}(t_e))$\vspace{0.1cm}

and transversality conditions
$\psi (t_e^*,\mbb{x}^*(t_e^*)) = [\psi_1(t_e^*,\mbb{x}^*(t_e^*)) \cdots \psi_p(t_e^*,\mbb{x}^*(t_e^*))]^T = \mbb{0}$ \\
$\frac{\partial}{\partial t_e}\phi (t_e^*, \mbb{x}^*(t_e^*),\mbb{\mu}^*) + H(t_e^*,\mbb{x}^*(t_e^*),\mbb{u}^*(t_e^*),\mbb{\lambda}^*(t_e^*)) = 0$\vspace{0.1cm}

with inequality constraints $G_k(t_e,\mbb{u})=\psi_k(t_e,\mbb{x}(t_e)) \leq 0$ then the first transversality condition becomes:\vspace{0.1cm}

$\psi_k (t_e, \mbb{x}(t_e))\leq 0$\\
$\mbb{\mu}^* \geq 0$ \\
$\mbb{\psi}^T(t_e, \mbb{x}(t_e))\mbb{\mu}^* = 0$\vspace{0.2cm}

\subsubsection{Reachability condition}
Regularity condition can be interpreted as a reachability condition. If it does not hold, it may not be possible to find a $\mbb{u}^*(t)$ to transfer the state $\mbb{x}(t)$ from initial to end state in finite time.

\subsection{Input constrained}
\verb!Hamiltonian! \\
$H(x,u,\bar{\lambda})=\bar{\lambda}\bar{f}(x,u)=[\bar{\lambda}_1\cdots\bar{\lambda}_n]f(x,u) + \bar{\lambda}_{n+1}l(x,u)$\vspace{0.1cm}

\subsubsection{Pontryagin maximum principle (autonomous systems)}
$\dot{\bar{\mbb{x}}}^*=(\nabla_{\bar{\lambda}} H)(\mbb{x}^*,\mbb{u}^*,\bar{\mbb{\lambda}}^*) = \begin{bmatrix} \mbb{f}((\mbb{x}^*,\mbb{u}^*) \\ l((\mbb{x}^*,\mbb{u}^*)\end{bmatrix}$\\ 
with $\bar{\mbb{x}}^*(t_0) = \begin{bmatrix} \mbb{x}_0 \\ 0 \end{bmatrix}, \,\, \mbb{x}^*(t_e) = \mbb{x}_e$ \\

$\dot{\bar{\mbb{\lambda}}}^*=-(\nabla_{\bar{x}} H)(\mbb{x}^*,\mbb{u}^*,\bar{\mbb{\lambda}}^*) = \begin{bmatrix} -(\nabla_{\bar{x}} H)(\mbb{x}^*,\mbb{u}^*,\bar{\mbb{\lambda}}^*) \\ 0 \end{bmatrix}$\vspace{0.1cm}

$\bar{\lambda}^*_{n+1} = const. \geq 0$ and $H(\mbb{x}^*,\mbb{u}^*,\bar{\mbb{\lambda}}^*) = const. \geq 0$ \vspace{0.1cm}

For free endtime $t_e$ - transversality condition: \\
$H(\mbb{x}^*(t_e^*), \mbb{u}^*(t_e^*), \bar{\mbb{\lambda}}^*(t_e^*))=0$\\

with target set condition $\rightarrow$ transversality condition: \\
$(\lambda^*(t_e^*))^T\mbb{d}=0, \,\, \forall \mbb{d} \in \mathcal{T}_{x^*(t_e^*)}X_{ta}$\vspace{0.1cm}

Normal case: If $\mbb{u}^*$ dependent of $l(\cdot)$, then set $\bar{\lambda}_{n+1}^*(t) =1$\\
Abnormal case: If $\mbb{u}^*$ independent of $l(\cdot)$, then $\bar{\lambda}_{n+1}^*(t) =0$\\
\vspace{0.2cm}

\subsubsection{Pontryagin maximum principle (non-autonomous systems)}
Same as autonomous systems, except that $\mbb{f}$ and $l$ depend on time and \\ 
$\frac{\partial}{\partial t}H(t_e^*, \mbb{x}^*,\mbb{u}^*,\bar{\mbb{\lambda}}^*) = (\bar{\mbb{\lambda}}^*)^T(\frac{\partial}{\partial t} \mbb{f})(t_e^*, \mbb{x}^*,\mbb{u}^*)$

\subsection{Nonlinear affine input systems}
$\dot{\mbb{x}}= \mbb{f}_0(\mbb{x}) + \sum_{j=1}^{m} \mbb{f}_j(\mbb{x})u_j, \,\, t>t_0, \,\, \mbb{x}(t_0) = \mbb{x}_0, \,\, u_j \in [u_j^-, u_j^+]$ 

\subsubsection{Cost functionals minimizing consumption}
Due to affine input structure the problem can be split into independent problems. \vspace{0.1cm}

$J(u) = \int_{t_0}^{t_e} (l_0(x(t)) + \textcolor{red}{\frac{1}{2}} \sum_{j=1}^{m} r_j |u_j(t)|^{\textcolor{red}{2}})dt$\\ $q_j(x,\lambda)=\lambda^T f_j(x)$\vspace{0.1cm}

Optimal control is defined as - singular case if $q_j(x,\lambda) = \pm r_j$: \\
$u_j^* = \begin{cases} u_j^- &q_j(x,\lambda) > r_j \\ 0 & q_j(x,\lambda) \in (-r_j, r_j) \\ u_j^+ & q_j(x,\lambda) < -r_j \end{cases}$

\subsubsection{Cost functionals addressing energy optimality}
Squared dependence of input in cost functional (\textcolor{red}{$^*$}). Can also be split into $m$ independent minimization problems.\vspace{0.1cm}

Optimal control is defined as, where $u_j^0=-\frac{q_j(\cdot)}{r_j}$.\\
$u_j^* = \begin{cases} u_j^- & u_j^0 \leq u_j^- \\ u_j^0 & u_j^0 \in (u_j^-, u_j^+) \\ u_j^+ & u_j^0 \geq u_j^+ \end{cases}$

\subsubsection{Cost functionals addressing time optimality}
Optimal control is either $u_j^-$ ($q_j(\cdot)>0$) or $u_j^+$ ($q_j(\cdot)<0$) where $q_j(\cdot)$ directly results from the minimization of the Hamiltonian (bang-bang control) - singular case if $q_j(\cdot)=0$ \\
$J(u) = \int_{t_0}^{t_e} 1 dt = t_e-t_0$ \\

\subsubsection{Singular case}
$\zeta^*(t)=(\nabla_u H)(t^*,x^*,u^*,\bar{\lambda}^*)$ \\
Determine $k$ by $\frac{\partial}{\partial u}\left[\frac{d^k}{dt^k}\zeta^*(t)\right] \neq 0$ and this also provides the optimal control $u^*$. \\
Then $(-1)^{\frac{k}{2}}\frac{\partial}{\partial u}\left[\frac{d^k}{dt^k}\zeta^*(t)\right] \geq 0$ has to be fulfilled for a minimum along the arc.

\section{Numerical solution of optimal control problems}
\subsection{Indirect methods}
Directly approach the 2-point BVP and solve:\\ 
$\dot{\mbb{x}} = (\nabla_{\lambda} H)(t,\mbb{x},\mbb{k}(\mbb{x},\mbb{\lambda}),\mbb{\lambda}), \,\,\, \mbb{x}^*(t_0) = \mbb{x}_0$, $\psi(t_e,\mbb{x}(t_e))=\mbb{0}$ \\
$\dot{\mbb{\lambda}}^* = -(\nabla_{x} H)(t,\mbb{x},\mbb{k}(\mbb{x},\mbb{\lambda}),\mbb{\lambda}), \,\,\, \mbb{\lambda}(t_e) = (\nabla_{x_e} \phi)(t_e, \mbb{x}(t_e),\mbb{\mu})$, \\ $\dot{\mbb{\mu}} = \mbb{0}$\\ 

+ prove insight into structure of optimal solution \\
+ determine highly accurate or even exact solution \\
- utilize adjoint variables for sensitivity analysis and controller design \\ 

\subsubsection{Discretization methods}
Discretizing $[t_0,t_e]$ into $N+1$ steps to approximate solution at discretization points using trapezoidal rule. Numerical solution by finding zeros of nonlinear algebraic system using e.g. Newtons method.\vspace{0.1cm}

+ numerically robust solution due to simultaneous consideration of differential equations and boundary conditions \\
+ special matrix structure can be exploited for numerical solution (reuse previous results $\mbb{F}(\cdot)$)\\
$\circ$ convergence relies on the initial guess of adjoint variables \\
$\circ$ number of discretization steps influences accuracy and computational burden \\

\subsubsection{Shooting method}
Replace BVP by IVP by guessing initial states $\mbb{\lambda}(t_0) = \mbb{\lambda}_0$ and $\mbb{\mu}$. Solve residual terms at boundary $t=t_e$ with Newton method to get iterative solution.\vspace{0.1cm}

+ Low implementation effort\\
$\circ$ Relies on proper choice of initial state $\mbb{\lambda}_0$\\
- canonical equations tend to be weakly stable, thus numerical issues may arise due to integration in large intervals. Use multiple shooting instead.
\subsubsection{Collocation methods}
Use linear combination of linear independent basis functions ((Legendre) polynomials) which fulfill the boundary conditions at $t=t_0$ and $t=t_e$. It is imposed that differential equations and boundary conditions are satisfied pointwise at $K+1$ distinct collocation points, to then determine the coefficients for the basis functions.
\subsubsection{Extension to free endtime}
Use time scaling so that $\frac{d}{dt}=\frac{1}{\nu}\frac{d}{d\tau}$ in order to transform E-L equations (or canonical equations) to a fixed time interval $\tau \in [0,1]$. Determination of optimal endtime $t_e^*$ reduces to find constant scaling factor $\nu>0$.

\subsection{Direct methods}
Discretize infinite-dimensional opt. prob. to obtain a finite-dimensional static optimization problem.
Assuming Bolzano form with fixed endtime and equality and inequality constraints. The time interval $[t_0,t_e]$ is discretized into $N+1$ stages and control inputs are parametrized in each subinterval with order of approximation by functions that are piecewise constant (sample and hold), piecewise linear (linear interpolation), etc. In practice: Lagrange polynomials. \vspace{0.1cm}

+ avoid determination of canonical equations \\
+ simpler incorporation of state and path constraints \\
+ compute Lagrange multiplier in post-processing step \\
+ improved convergence behaviour \\
+ allow solving of optimal control problems for systems with ODE, differential-algebraic equations and PDE \\
- only suboptimal solution \\
\subsubsection{Direct sequential methods}
Problem reduces to static optimization problem.\vspace{0.1cm}

+ Use SQP for solving nonlinear optimization problem. \\
$\circ$ Accuracy depends on the used numerical solver and is independent of the time grid. \\
- Problems may arise with unstable systems or differential equations with no solution for certain decision variables.\\

\subsubsection{Direct simultaneous methods}
Discretized in the control and state variables. \vspace{0.1cm}

$\circ$ Differential equations are fulfilled at converged solution $\hat{\mbb{x}}^*, \hat{\mbb{u}}^*$ only. \\
$\circ$ Inequality constraints are satisfied at discretization points $t^j$ only.\\
$\circ$ Number of time intervals influences approximation of optimal control and accuracy of approxmiation of the solution to the differential equations.
